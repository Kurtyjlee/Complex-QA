{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to scrap new platforms using newsAPI\n",
    "\n",
    "Documentation: https://newsapi.org/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from newsapi import NewsApiClient\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "newsapi = NewsApiClient(api_key=\"api_key_here\")\n",
    "\n",
    "domain = \"channelnewsasia.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting list of article urls from newsapi\n",
    "\"\"\"\n",
    "\n",
    "list_of_articles = []\n",
    "for i in range(1, 6):\n",
    "    print(f\"working on page: {i}\")\n",
    "    all_articles = newsapi.get_everything(\n",
    "        domains=domain, language=\"en\", sort_by=\"relevancy\", page=i\n",
    "    )\n",
    "    list_of_articles += all_articles[\"articles\"]\n",
    "list_of_urls = list(map(lambda item: item[\"url\"], list_of_articles))\n",
    "print(len(list_of_articles))\n",
    "print(list_of_articles)\n",
    "print(list_of_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping CNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CNA\n",
    "API_KEY = \"api key here\"\n",
    "\n",
    "\n",
    "# Using scrapeops to bypass forbidden restriction on CNA\n",
    "def get_scrapeops_url(url:str) -> str:\n",
    "    \"\"\"\n",
    "    get the scrapops url from the article url\n",
    "\n",
    "    Args:\n",
    "        url (str): article url\n",
    "\n",
    "    Returns:\n",
    "        str: scrapops url\n",
    "    \"\"\"\n",
    "    payload = {\"api_key\": API_KEY, \"url\": url}\n",
    "    proxy_url = \"https://proxy.scrapeops.io/v1/?\" + urlencode(payload)\n",
    "    return proxy_url\n",
    "\n",
    "\n",
    "# for channelnewsasia\n",
    "articles = []\n",
    "\n",
    "\n",
    "# Method to get content out of the articles\n",
    "def get_article_content(article_url:str) -> None:\n",
    "    \"\"\"\n",
    "    Getting article content and storing it in the articles list\n",
    "\n",
    "    Args:\n",
    "        article_url (str): article url\n",
    "    \"\"\"\n",
    "    r = requests.get(get_scrapeops_url(article_url))\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    print(f\"Working on {counter}: {article_url}\")\n",
    "\n",
    "    try:\n",
    "        headline = soup.find(\"h1\", class_=\"h1 h1--page-title\").text.strip()\n",
    "\n",
    "        # Getting text from all p tags\n",
    "        content = map(lambda c: c.text, soup.find_all(\"p\", class_=None))\n",
    "        content_text = \" \".join([str(item) for item in content])\n",
    "\n",
    "        # Getting links\n",
    "        all_links = list(\n",
    "            map(\n",
    "                lambda tag: tag[\"href\"].strip(),\n",
    "                soup.find_all(\"a\", class_=\"h6__link list-object__heading-link\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        articles.append(\n",
    "            {\"title\": headline, \"content\": content_text, \"links\": all_links}\n",
    "        )\n",
    "        print(\"task completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for: {article_url}\")\n",
    "        print(e)\n",
    "\n",
    "\n",
    "counter = 1\n",
    "for article_url in list_of_urls:\n",
    "    if counter == 100:\n",
    "        break\n",
    "    get_article_content(article_url)\n",
    "    counter += 1\n",
    "\n",
    "# Print articles once done\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Straitstimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For straits times\n",
    "articles = []\n",
    "for article_url in list_of_urls:\n",
    "    html = request.urlopen(article_url).read().decode(\"utf8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    print(f\"Working on: {article_url}\")\n",
    "\n",
    "    try:\n",
    "        # Getting headline, content and connecting url\n",
    "        headline = soup.find(\"h1\", class_=\"headline node-title\").text.strip()\n",
    "\n",
    "        # Getting text from all p tags\n",
    "        content = map(lambda c: c.text, soup.find_all(\"p\", class_=None))\n",
    "        content_text = \" \".join([str(item) for item in content])\n",
    "\n",
    "        # Getting links\n",
    "        all_links = list(map(lambda tag: tag['href'].strip(), soup.select('p a[href]')))\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": headline, \n",
    "            \"content\": content_text,\n",
    "            \"links\": all_links\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for: {article_url}\")\n",
    "        print(e)\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the articles in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles)\n",
    "filename = \"newsapi-CNA.csv\"\n",
    "df = pd.DataFrame.from_records(articles)\n",
    "df.to_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa-dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
