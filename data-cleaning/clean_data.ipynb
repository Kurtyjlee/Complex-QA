{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "source_path = \"../data/vicuna_generations/rsis/answers\"\n",
    "source_filenames = [\n",
    "    \"rsis_vicuna-13b-v1_vicuna-7b-v1.1_temp1_answers.json\",\n",
    "    \"rsis_vicuna-13b-v1_vicuna-13b-v1.1_temp1_answers.json\",\n",
    "    \"rsis_vicuna-13b-v1_gpt-3.5-turbo_temp1_answers.json\"\n",
    "]\n",
    "target_path = \"../data/vicuna_generations/rsis/questions\"\n",
    "target_filename = \"rsis_vicuna-13b-v1.1_100_questions.json\"\n",
    "\n",
    "source_filename = source_filenames[0]\n",
    "\n",
    "# Getting json from a file\n",
    "def open_file(path:str, filename:str) -> list[dict]:\n",
    "\n",
    "\n",
    "    # Opening source file\n",
    "    with open(f\"{path}/{filename}\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_file(path:str, filename:str, data:list[dict]) -> None:\n",
    "\n",
    "    with open(f\"{path}/{filename}\", \"w\") as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_target_data(question:str, target_data:list[dict]) -> int:\n",
    "    for i in range(len(target_data)):\n",
    "        if question in target_data[i][\"output\"]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def split_context(context:str) -> list[str]:\n",
    "    context_list = context.split(\". \")\n",
    "    context_list = [sentence + \".\" for sentence in context_list]\n",
    "    return context_list\n",
    "\n",
    "def match_source_data(source_data:list[dict], target_data:list[dict]) -> list[dict]:\n",
    "    for data in source_data:\n",
    "        question = data[\"question\"]\n",
    "\n",
    "        result = match_target_data(question, target_data)\n",
    "\n",
    "        # If result is valid\n",
    "        if (result != -1):\n",
    "            data[\"context\"] = split_context(target_data[result][\"context\"].strip())\n",
    "            data[\"output\"] = [x for x in data[\"output\"] if x.strip() != \"\"]\n",
    "\n",
    "    return source_data\n",
    "\n",
    "target_data = open_file(target_path, target_filename)\n",
    "\n",
    "for filename in source_filenames:\n",
    "    source_data = open_file(source_path, filename)\n",
    "    source_data = match_source_data(source_data, target_data)\n",
    "    save_file(source_path, filename, source_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import glob\n",
    "import json\n",
    "from QaGeneration import ensure_string\n",
    "\n",
    "def is_question(keyword:str, data:Dict) -> bool:\n",
    "    if keyword in data and \"?\" in data[keyword]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_list_match(keyword:str, target_list:List[str], data:Dict) -> bool:\n",
    "    if keyword in data:\n",
    "        text = ensure_string(data[keyword])\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    for target in target_list:\n",
    "        if target in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def contains_blank(data:Dict) -> bool:\n",
    "    for key, value in data.items():\n",
    "        if type(value) == str and value == \"\":\n",
    "            return True\n",
    "        elif type(value) == int and value == 0:\n",
    "            return True\n",
    "        elif type(value) == float and value == 0.0:\n",
    "            return True\n",
    "        elif isinstance(value, List) and 0 in value:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_file_path(file_directories: List[str]) -> List[str]:\n",
    "    result_list = []\n",
    "    for file_directory in file_directories:\n",
    "        result_list += glob.glob(f\"{file_directory}/*.json\")\n",
    "    return result_list\n",
    "\n",
    "file_directories = [\n",
    "    # \"../data/generations/nyt\",\n",
    "    \"../data/generations/rsis\",\n",
    "    # \"../data/generations/straitstimes\"\n",
    "]\n",
    "\n",
    "file_paths = get_file_path(file_directories)\n",
    "\n",
    "flagged_words = [\n",
    "    \"I'm sorry\",\n",
    "    \"as an AI language model\"\n",
    "]\n",
    "\n",
    "keyword_list = [\n",
    "    \"point_form_close_book_answer\",\n",
    "    \"close_book_answer\"\n",
    "]\n",
    "\n",
    "\n",
    "for file in file_paths:\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    new_dataset = [data for data in dataset if is_question(keyword=\"question\", data=data)]\n",
    "    new_dataset = [data for data in dataset if not contains_blank(data)]\n",
    "    for keyword in keyword_list:\n",
    "        new_dataset = [data for data in dataset if not is_list_match(keyword=keyword, target_list=flagged_words, data=data)]\n",
    "\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(new_dataset, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_instruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
