[
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using large \"instruction-tuned\" language models that are finetuned to respond to instructions, as compared to other language models?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate instruction, input, and output samples from a language model",
      "C) To curate a set of expert-written instructions for novel tasks",
      "D) To demonstrate the performance of pretrained language models on SUPERNATURALINSTRUCTIONS",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model called SELF-INSTRUCT",
      "* The ability of large language models to generalize to new tasks",
      "* The limitations of using human-written instruction data for pre-training language models",
      "* The demonstration of improved performance on a specific task using SELF-INSTRUCT",
      "",
      "What is the main benefit of SELF-INSTRUCT?",
      "",
      "* It allows for the alignment of pre-trained language models with instructions",
      "* It requires less human annotation than traditional methods",
      "* It provides an almost annotation-free method for aligning pre-trained language models with instructions",
      "* It allows for the creation of a large synthetic dataset for future studies on instruction tuning",
      "",
      "What is the main limitation of using existing public instruction datasets for instruction tuning?",
      "",
      "* The limited quantity, diversity, and creativity of the instruction data",
      "* The challenge of annotating large-scale instruction data for humans",
      "* The performance gap between using existing public instruction datasets and using private user data and human annotations",
      "* The need for expertise to write labeled instances for each task"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "Answer: The main purpose of the SELF-INSTRUCT framework is to improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations, without relying on limited human-written instruction data."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using a large \"instruction-tuned\" language model that has been finetuned to respond to instructions, as compared to a regular language model?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study described in the input passage?",
      "",
      "A) The limitations of using large language models for instruction-tuning",
      "B) The success of instruction-tuning large language models",
      "C) The process of generating instruction-tuning data for large language models",
      "D) The benefits of using human feedback for instruction-tuning large language models",
      "",
      "Correct answer: B) The success of instruction-tuning large language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "A. The limitations of pretrained language models",
      "B. The success of instruction-tuning large language models",
      "C. The introduction of a new framework for improving instruction-following capabilities",
      "D. The use of human feedback in instruction-tuning large language models",
      "",
      "Correct answer: B. The success of instruction-tuning large language models",
      "",
      "Explanation: The passage discusses the success of instruction-tuning large language models and introduces a new framework called SELF-INSTRUCT that improves the instruction-following capabilities of pretrained language models. The main focus of the passage is on the success of instruction-tuning large language models.",
      "",
      "A. The limitations of pretrained language models",
      "B. The success of instruction-tuning large language models",
      "C. The introduction of a new framework for improving instruction-following capabilities",
      "D. The use of human feedback in instruction-tuning large language models",
      "",
      "Correct answer: B. The success of instruction-tuning large language models",
      "",
      "Explanation: The passage discusses the success of instruction-tuning large language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the paper \"SELF-INSTRUCT: Aligning Pretrained Language Models with Instructions with Limited Human Feedback\"?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate instruction, input, and output samples from a language model",
      "C) To curate a set of expert-written instructions for novel tasks",
      "D) To demonstrate the performance of InstructGPT001 on SUPERNATURALINSTRUCTIONS",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "(A) To improve the instruction-following capabilities of pretrained language models",
      "(B) To generate instruction, input, and output samples from a language model",
      "(C) To curate a set of expert-written instructions for novel tasks",
      "(D) To demonstrate the performance of pretrained language models on SUPERNATURALINSTRUCTIONS",
      "(E) To release a large synthetic dataset for future studies on instruction tuning",
      "",
      "Correct answer: (A) To improve the instruction-following capabilities of pretrained language models."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model called SELF-INSTRUCT",
      "* The limitations of using large-scale human-written instruction data for pre-training language models",
      "* The ability of pre-trained language models to generalize to new tasks",
      "* The release of a large synthetic dataset for future studies on instruction tuning"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study described in the input passage?",
      "",
      "A) The ability of large language models to generalize to new tasks",
      "B) The limitations of using human-written instruction data to improve the instruction-following capabilities of pretrained language models",
      "C) The introduction of a new framework for improving the instruction-following capabilities of pretrained language models",
      "D) The success of pretrained language models in following instructions",
      "",
      "Correct answer: B) The limitations of using human-written instruction data to improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate instruction, input, and output samples from a language model",
      "C) To align pretrained language models with instructions",
      "D) To curate a set of expert-written instructions for novel tasks",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "A. The benefits of using large language models",
      "B. The limitations of using large language models",
      "C. The introduction of a new framework for improving language model performance",
      "D. The success of pre-training language models on a large dataset",
      "",
      "Answer: B. The limitations of using large language models",
      "",
      "Explanation: The passage discusses the limitations of using large language models, particularly in terms of their dependence on human-written instruction data. It introduces a new framework called SELF-INSTRUCT that aims to improve the instruction-following capabilities of pretrained language models by generating instruction, input, and output samples from the model itself. The passage also mentions that the success of pre-training language models on a large dataset is not a guarantee of their ability to generalize to new tasks, and that the limitations of using large language models are a key motivation for the development of SELF-INSTRUCT. Therefore, option B is the correct answer."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study discussed in the passage?",
      "",
      "A) The limitations of using large language models for instruction-tuning",
      "B) The success of instruction-tuning large language models",
      "C) The process of generating instruction-tuning data for large language models",
      "D) The benefits of using private user data for instruction-tuning large language models",
      "",
      "Correct answer: B) The success of instruction-tuning large language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study discussed in the passage?",
      "",
      "A. The limitations of instruction-tuned language models",
      "B. The success of instruction-tuned language models on new tasks",
      "C. The process of generating instruction-tuned language models",
      "D. The use of human feedback in instruction-tuned language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using a large \"instruction-tuned\" language model that has been finetuned to respond to instructions, as compared to a regular language model?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using large \"instruction-tuned\" language models that are finetuned to respond to instructions, as compared to other methods for improving the instruction-following capabilities of pretrained language models?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using self-instructed language models for instruction-tuning, as described in the passage?",
      "",
      "A. They require less human feedback during the tuning process.",
      "B. They can generalize to new tasks without the need for human-written instruction data.",
      "C. They outperform pretrained language models that are finetuned with human-written instruction data.",
      "D. They are limited in the quantity, diversity, and creativity of the instruction data they can use.",
      "",
      "Correct answer: C. They can generalize to new tasks without the need for human-written instruction data."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model",
      "* The benefits of using pre-trained language models for instruction-tuning",
      "* The limitations of using pre-trained language models for instruction-tuning",
      "* The proposed solution to improve the instruction-following capabilities of pre-trained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "A) The limitations of pretrained language models",
      "B) The success of instruction-tuning large language models",
      "C) The benefits of using synthetic datasets for instruction tuning",
      "D) The process of generating instruction-tuning data",
      "",
      "Correct answer: B) The success of instruction-tuning large language models",
      "",
      "Explanation: The passage discusses the success of instruction-tuning large language models, which is the main focus of the passage. The passage also discusses the limitations of pretrained language models, the benefits of using synthetic datasets for instruction tuning, and the process of generating instruction-tuning data, but these topics are secondary to the main focus of the passage."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "* To improve the instruction-following capabilities of pretrained language models",
      "* To generate instruction, input, and output samples from a language model",
      "* To prune samples before using them to finetune the original model",
      "* To align pretrained language models with instructions",
      "* To release a large synthetic dataset for future studies on instruction tuning",
      "",
      "What is the main advantage of SELF-INSTRUCT compared to existing public instruction datasets?",
      "",
      "* It provides an almost annotation-free method for aligning pretrained language models with instructions",
      "* It outperforms existing public instruction datasets by a large margin",
      "* It leaves only a 5% absolute gap behind InstructGPT001",
      "* It requires less human feedback to achieve better performance",
      "* It requires expertise for writing labeled instances for each task",
      "",
      "What is the role of human feedback in the success of instruction-tuning LMs?",
      "",
      "* It is a necessary and indispensable aspect of instruction-tuning as LMs need to learn about issues that were not quite learned during pre-"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using self-instructed language models for instruction-tuning compared to models trained with private user data and human annotations?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model",
      "* The benefits of using pre-trained language models for instruction-tuning",
      "* The limitations of using pre-trained language models for instruction-tuning",
      "* The authors' process for improving instruction-tuning capabilities of pre-trained language models",
      "",
      "What is the main advantage of using pre-trained language models for instruction-tuning?",
      "",
      "* They require less human feedback",
      "* They are more familiar with language instructions",
      "* They can generalize to new tasks without human annotation",
      "* They are more creative than human-written instruction data",
      "",
      "What is the name of the framework introduced in the passage for improving instruction-tuning capabilities of pre-trained language models?",
      "",
      "* SELF-INSTRUCT",
      "* InstructGPT001",
      "* GPT3",
      "* Pretrained language models",
      "",
      "What is the main advantage of SELF-INSTRUCT over existing public instruction datasets?",
      "",
      "* It provides an almost annotation-free method for aligning pre-trained language models with instructions",
      "* It improves the performance of"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using a large \"instruction-tuned\" language model that has been finetuned to respond to instructions?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "A) The limitations of using large language models for instruction-tuning",
      "B) The success of instruction-tuning large language models",
      "C) The introduction of a new framework for improving instruction-tuning capabilities",
      "D) The challenges of generating large-scale instruction data",
      "",
      "Answer: B) The success of instruction-tuning large language models",
      "",
      "Explanation: The passage discusses the success of instruction-tuning large language models, particularly the method introduced in SELF-INSTRUCT, which improves the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. The passage also describes the results of applying the method to vanilla GPT-3, which demonstrates a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001, which is trained with private user data and human annotations. The passage also discusses the limitations of using large language models for instruction-tuning and the challenges of generating large-scale instruction data. Therefore, the main focus of the"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model",
      "* The ability of large language models to generalize to new tasks",
      "* The limitations of using human-written instruction data for pre-training language models",
      "* The SELF-INSTRUCT framework for improving instruction-following capabilities of pretrained language models",
      "",
      "What is the purpose of the SELF-INSTRUCT framework?",
      "",
      "* To generate instruction, input, and output samples from a language model",
      "* To improve the performance of pretrained language models on a specific task",
      "* To align pretrained language models with instructions",
      "* To create a large synthetic dataset for future studies on instruction tuning",
      "",
      "What is the main advantage of the SELF-INSTRUCT framework compared to existing public instruction datasets?",
      "",
      "* It provides almost annotation-free method for aligning pretrained language models with instructions",
      "* It allows for a larger quantity, diversity, and creativity of instruction data",
      "* It provides a large margin of improvement over existing public instruction datasets",
      "* It requires less human feedback for aligning pretraining objective",
      "",
      "What is the main conclusion"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using large \"instruction-tuned\" language models for instruction following?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "* To improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations.",
      "",
      "Why does SELF-INSTRUCT work?",
      "",
      "* It is worthwhile to reflect on the role that high-quality human feedback plays in enabling the recent successes on instruction-tuning LMs."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "Answer: The main purpose of the SELF-INSTRUCT framework is to improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model",
      "* The ability of pretrained language models to generalize to new tasks",
      "* The limitations of using human-written instruction data for pretrained language models",
      "* The introduction of a new framework for improving instruction-following capabilities of pretrained language models",
      "",
      "Answer: The main focus of the passage is the introduction of a new framework for improving instruction-following capabilities of pretrained language models.",
      "",
      "Is the information in the passage unambiguous?",
      "",
      "* Yes",
      "* No",
      "",
      "Answer: Yes",
      "",
      "Is the information in the passage answerable from the passage?",
      "",
      "* Yes",
      "* No",
      "",
      "Answer: Yes",
      "",
      "Is the answer to the question unique to the passage?",
      "",
      "* Yes",
      "* No",
      "",
      "Answer: Yes",
      "",
      "Does the answer use text phrases from the passage?",
      "",
      "* Yes",
      "* No",
      "",
      "Answer: Yes"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using large \"instruction-tuned\" language models that are finetuned to respond to instructions?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using large \"instruction-tuned\" language models that are finetuned to respond to instructions, as compared to other language models?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "Answer: The main purpose of the SELF-INSTRUCT framework is to improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations, thereby enabling the language model to generalize to new tasks without relying heavily on human-written instruction data."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study discussed in the input passage?",
      "",
      "a) The ability of large language models to generalize to new tasks",
      "b) The use of human feedback in instruction-tuning of language models",
      "c) The introduction of a new framework for improving instruction-following capabilities of pretrained language models",
      "d) The performance of pretrained language models on a specific task",
      "",
      "Answer: b) The use of human feedback in instruction-tuning of language models",
      "",
      "Explanation: The input passage discusses the use of human feedback in instruction-tuning of large language models, specifically focusing on the SELF-INSTRUCT framework which generates instruction, input, and output samples from a language model and prunes them before using them to finetune the original model. Therefore, the question should focus on the main focus of the study, which is the use of human feedback in instruction-tuning of language models.",
      "",
      "Option a) is incorrect because the passage does not discuss the ability of large language models to generalize to new tasks.",
      "",
      "Option c) is incorrect because the passage does not introduce a new framework for improving instruction-following"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using large \"instruction-tuned\" language models that are finetuned to respond to instructions?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model called SELF-INSTRUCT",
      "* The ability of large \"instruction-tuned\" language models to generalize to new tasks",
      "* The limitations of using human-written instruction data to improve the instruction-following capabilities of pretrained language models",
      "* The demonstration of a 33% improvement in performance on SUPERNATURALINSTRUCTIONS using SELF-INSTRUCT",
      "",
      "Correct answer: The limitations of using human-written instruction data to improve the instruction-following capabilities of pretrained language models",
      "",
      "Explanation: The passage discusses the limitations of using human-written instruction data to improve the instruction-following capabilities of pretrained language models, particularly for larger models. This is the main focus of the passage."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using a large \"instruction-tuned\" language model that has been finetuned to respond to instructions, compared to a regular language model?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the paper \"SELF-INSTRUCT: Aligning Pretrained Language Models with Instructions by Generating Tasks with a Vanilla Pretrained Language Model Itself\"?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The limitations of large language models",
      "* The success of instruction-tuning large language models",
      "* The process of generating instruction-tuned language models",
      "* The benefits of aligning pretrained language models with instructions"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "* To improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations.",
      "",
      "What is the method used in SELF-INSTRUCT to generate instruction, input, and output samples?",
      "",
      "* Generating instruction, input, and output samples from a language model, then pruning them before using them to finetune the original model.",
      "",
      "What is the result of applying SELF-INSTRUCT to vanilla GPT3?",
      "",
      "* A 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001, which is trained with private user data and human annotations.",
      "",
      "What is the main advantage of SELF-INSTRUCT compared to existing public instruction datasets?",
      "",
      "* It provides an almost annotation-free method for aligning pretrained language models with instructions.",
      "",
      "What is the name of the large synthetic dataset used in SELF-INSTRUCT to facilitate future studies"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "(A) To improve the instruction-following capabilities of pretrained language models",
      "(B) To generate instruction, input, and output samples from a language model",
      "(C) To demonstrate the performance of InstructGPT001 on SUPERNATURALINSTRUCTIONS",
      "(D) To curate a set of expert-written instructions for novel tasks",
      "(E) To provide an almost annotation-free method for aligning pretrained language models with instructions",
      "",
      "(E) To provide an almost annotation-free method for aligning pretrained language models with instructions"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using a large \"instruction-tuned\" language model that has been finetuned to respond to instructions, as compared to a regular language model?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using self-instructed language models for instruction-tuning, as described in the passage?",
      "",
      "A. They require less human annotation.",
      "B. They can generalize to new tasks without human instruction.",
      "C. They are trained on a diverse and creative dataset of human-written instructions.",
      "D. They outperform instruction-tuned models that are trained on private user data and human annotations.",
      "",
      "Correct answer: D. They outperform instruction-tuned models that are trained on private user data and human annotations."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study discussed in the input passage?",
      "",
      "A) The limitations of using large-scale human annotation for instruction-tuning large language models",
      "B) The success of instruction-tuning large language models with limited human annotation",
      "C) The use of synthetic instruction data for improving the performance of pretrained language models",
      "D) The role of creativity and expertise in human annotation of large-scale instruction data",
      "",
      "Correct answer: B) The success of instruction-tuning large language models with limited human annotation"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the paper \"SELF-INSTRUCT: Aligning Pretrained Language Models with Instructions via Generating Tasks with a Vanilla Pretrained Language Model Itself\"?",
      "A) The introduction of a new language model architecture",
      "B) The use of self-generated instructions for improving pretrained language models",
      "C) The comparison of different instruction-tuning methods",
      "D) The limitations of using human-written instruction data for pretrained language models",
      "",
      "D) The limitations of using human-written instruction data for pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The limitations of large language models",
      "* The benefits of using self-generated instruction data for improving language models",
      "* The introduction of a new framework for improving instruction-following capabilities of pretrained language models",
      "* The demonstration of a 33% improvement in performance on a task using the SELF-INSTRUCT framework",
      "",
      "What is the main goal of the SELF-INSTRUCT framework?",
      "",
      "* To generate instruction, input, and output samples from a language model",
      "* To improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations",
      "* To align pretrained language models with instructions",
      "* To provide an almost annotation-free method for aligning pretrained language models with instructions",
      "",
      "What is the process described in the passage for generating instruction-tuning data for SELF-INSTRUCT?",
      "",
      "* Generating instruction, input, and output samples from a language model",
      "* Pruning the samples before using them to finetune the original model",
      "* Conducting instruction tuning with this generated data",
      "* Using existing"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the paper \"SELF-INSTRUCT: Aligning Pretrained Language Models with Instructions Using Self-Generated Data\"?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study described in the input passage?",
      "",
      "A) The benefits of using large language models for instruction-tuning",
      "B) The limitations of using large language models for instruction-tuning",
      "C) The introduction of a new framework for improving the instruction-following capabilities of pretrained language models",
      "D) The success of pretrained language models in generalizing to new tasks",
      "",
      "Correct answer: B) The limitations of using large language models for instruction-tuning",
      "",
      "Explanation: The input passage describes a study that focuses on the limitations of using large language models for instruction-tuning. Therefore, the correct answer is B) The limitations of using large language models for instruction-tuning.",
      "",
      "A) The benefits of using large language models for instruction-tuning is mentioned in the passage, but it is not the main focus.",
      "",
      "C) The introduction of a new framework for improving the instruction-following capabilities of pretrained language models is mentioned in the passage, but it is not the main focus.",
      "",
      "D) The success of pretrained language models in generalizing to new tasks is mentioned in the passage, but"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model",
      "* The ability of large language models to generalize to new tasks",
      "* The limitations of using human-written instruction data for pre-training language models",
      "* The SELF-INSTRUCT framework for improving instruction-following capabilities of pre-trained language models",
      "",
      "What is the main advantage of the SELF-INSTRUCT framework?",
      "",
      "* It requires less human annotation",
      "* It allows for the generation of instruction, input, and output samples",
      "* It improves the performance of pre-trained language models on novel tasks",
      "* It aligns pre-trained language models with instructions without the need for human feedback",
      "",
      "What is the main conclusion of the passage?",
      "",
      "* The SELF-INSTRUCT framework is effective for improving the instruction-following capabilities of pre-trained language models",
      "* The success of the SELF-INSTRUCT framework is due to the use of high-quality human feedback",
      "* The limitations of the SELF-INSTRUCT framework are due to the use of a vanilla pre-tra"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "Answer: The main purpose of the SELF-INSTRUCT framework is to improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations, without relying on limited human-written instruction data. The framework generates instruction, input, and output samples from a language model, prunes them, and uses them to finetune the original model. The authors of the paper claim that their method outperforms existing public instruction datasets and is almost annotation-free."
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study discussed in the input passage?",
      "",
      "A) The limitations of instruction-tuning large language models",
      "B) The success of instruction-tuning large language models",
      "C) The process of aligning pretrained language models with instructions",
      "D) The use of human feedback in instruction-tuning large language models",
      "",
      "Correct answer: B) The success of instruction-tuning large language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The introduction of a new language model called SELF-INSTRUCT",
      "* The limitations of using pre-trained language models for instruction-tuning",
      "* The process of generating instruction, input, and output samples for pre-trained language models",
      "* The evaluation of the performance of SELF-INSTRUCT on various tasks",
      "",
      "What is the main advantage of using SELF-INSTRUCT for instruction-tuning pre-trained language models?",
      "",
      "* It requires less human annotation compared to traditional methods",
      "* It allows for better generalization of pre-trained language models to new tasks",
      "* It does not rely on limited and diverse instruction data",
      "* It provides an almost annotation-free method for aligning pre-trained language models with instructions",
      "",
      "What is the main conclusion of the passage regarding the success of SELF-INSTRUCT?",
      "",
      "* It outperforms existing public instruction datasets by a large margin",
      "* It requires less human feedback for successful instruction-tuning of pre-trained language models",
      "* It is challenging to align pre-trained language models with instructions"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the paper \"SELF-INSTRUCT: Aligning Pretrained Language Models with Instructions with Limited Human Annotation\"?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate instruction, input, and output samples from a language model",
      "C) To curate a set of expert-written instructions for novel tasks",
      "D) To demonstrate the performance of pretrained language models on SUPERNATURALINSTRUCTIONS",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study described in the input passage?",
      "",
      "A) The ability of large language models to generalize to new tasks",
      "B) The use of human feedback in instruction-tuning of language models",
      "C) The performance of pretrained language models on SUPERNATURALINSTRUCTIONS",
      "D) The limitations of existing public instruction datasets for instruction-tuning of language models",
      "",
      "Correct answer: B) The use of human feedback in instruction-tuning of language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "* To improve the instruction-following capabilities of pretrained language models",
      "* To generate instruction, input, and output samples from a language model",
      "* To prune samples before using them to finetune the original model",
      "* To align the language model to follow instructions better",
      "",
      "What is the main advantage of SELF-INSTRUCT over existing public instruction datasets?",
      "",
      "* It provides an almost annotation-free method for aligning pretrained language models with instructions",
      "* It outperforms using existing public instruction datasets by a large margin",
      "* It leaves only a 5% absolute gap behind InstructGPT001",
      "* It provides a large synthetic dataset to facilitate future studies on instruction tuning",
      "",
      "What is the role of high-quality human feedback in enabling the recent successes on instruction-tuning LMs?",
      "",
      "* It is a necessary and indispensable aspect of instruction-tuning as LMs need to learn about issues that were not quite learned during pre-training",
      "* It is an optional aspect of instruction-tuning as LM"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using a large \"instruction-tuned\" language model for instruction-following tasks?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the passage?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "(i) To improve the instruction-following capabilities of pretrained language models",
      "(ii) To generate instruction, input, and output samples from a language model",
      "(iii) To demonstrate a 33% improvement in performance on SUPERNATURALINSTRUCTIONS",
      "(iv) To align pretrained language models with instructions without human annotation",
      "",
      "Correct answer: (i) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate large-scale instruction data for instruction-tuning LMs",
      "C) To align pretrained language models with instructions",
      "D) To evaluate the performance of pretrained language models on novel tasks",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate instruction, input, and output samples from a language model",
      "C) To align pretrained language models with instructions",
      "D) To curate a set of expert-written instructions for novel tasks",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study discussed in the input passage?",
      "",
      "a) The performance of pretrained language models on zero-shot tasks",
      "b) The impact of human feedback on the instruction-tuning capabilities of pretrained language models",
      "c) The introduction of a new framework for improving the instruction-following capabilities of pretrained language models",
      "d) The limitations of existing public instruction datasets for instruction-tuning",
      "",
      "Correct answer: b) The impact of human feedback on the instruction-tuning capabilities of pretrained language models",
      "",
      "Explanation: The main focus of the study discussed in the input passage is the impact of human feedback on the instruction-tuning capabilities of pretrained language models. The passage discusses a study that introduces a new framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations, and demonstrates a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001, which is trained with private user data and human annotations. The passage also discusses the"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate instruction, input, and output samples from a language model",
      "C) To curate a set of expert-written instructions for novel tasks",
      "D) To demonstrate the performance of pretrained language models on SUPERNATURALINSTRUCTIONS",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The limitations of using large language models for instruction-tuning",
      "* The benefits of using self-instruction for language models",
      "* The process of generating instruction data for language models",
      "* The performance of pretrained language models on various tasks"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "A) To improve the instruction-following capabilities of pretrained language models",
      "B) To generate large-scale instruction data for instruction-tuning LMs",
      "C) To align pretrained language models with instructions",
      "D) To replace the need for human feedback in instruction-tuning LMs",
      "",
      "Correct answer: A) To improve the instruction-following capabilities of pretrained language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the paper \"SELF-INSTRUCT: Aligning Pretrained Language Models with Instructions by Generating Tasks with a Vanilla Pretrained Language Model Itself\"?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main advantage of using large \"instruction-tuned\" language models for instruction-following tasks?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the SELF-INSTRUCT framework?",
      "",
      "* To improve the instruction-following capabilities of pretrained language models by bootstrapping off its own generations.",
      "* To generate instruction, input, and output samples from a language model and use them to finetune the original model.",
      "* To provide an almost annotation-free method for aligning pretrained language models with instructions.",
      "* To release a large synthetic dataset to facilitate future studies on instruction tuning.",
      "",
      "What is the main advantage of SELF-INSTRUCT over existing public instruction datasets?",
      "",
      "* SELF-INSTRUCT provides an almost annotation-free method for aligning pretrained language models with instructions.",
      "* SELF-INSTRUCT demonstrates a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001.",
      "* SELF-INSTRUCT provides a large synthetic dataset to facilitate future studies on instruction tuning.",
      "* SELF-INSTRUCT aligns"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main purpose of the passage?",
      "",
      "* To introduce a new framework for improving the instruction-following capabilities of pretrained language models",
      "* To discuss the limitations of existing instruction-tuning methods",
      "* To describe the process of generating tasks with a vanilla pretrained language model and conducting instruction tuning with this generated data",
      "* To present a large synthetic dataset for future studies on instruction tuning",
      "",
      "What is the main advantage of the proposed framework, SELF-INSTRUCT, over existing methods?",
      "",
      "* It requires less human annotation",
      "* It allows for better generalization of pretrained language models to new tasks",
      "* It provides a nearly annotation-free method for aligning pretrained language models with instructions",
      "* It requires less pre-training data",
      "",
      "What is the main conclusion of the authors regarding the role of human feedback in instruction-tuning LMs?",
      "",
      "* Human feedback is necessary and indispensable for instruction-tuning as LMs need to learn about issues that were not quite learned during pre-training",
      "* Human feedback is an optional aspect of instruction-tuning as LMs are already quite familiar with"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the paper \"SELF-INSTRUCT: Aligning Pretrained Language Models with Instructions without Human Annotations\"?"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the study described in the input passage?",
      "",
      "A) The performance of large language models on novel tasks",
      "B) The use of human feedback in instruction-tuning of language models",
      "C) The limitations of pre-training language models on limited instruction data",
      "D) The release of a large synthetic dataset for instruction tuning",
      "",
      "Correct answer: B) The use of human feedback in instruction-tuning of language models"
    ]
  },
  {
    "definition": [
      "Given a passage, construct a question on the basis of the information present in the passage. Construct the question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) its answer is unique (iv) it answer uses text phrases from the passage. Avoid creating questions that (i) can't be answered correctly without actually understanding the passage and (ii) uses the same words or phrases given in the passage."
    ],
    "context": [
      "Large \u201cinstruction-tuned\u201d language models\n",
      "(finetuned to respond to instructions) have\n",
      "demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless,\n",
      "they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the\n",
      "generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities\n",
      "of pretrained language models by bootstrapping off its own generations. Our pipeline\n",
      "generates instruction, input, and output samples from a language model, then prunes\n",
      "them before using them to finetune the original model. Applying our method to vanilla\n",
      "GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001\n",
      ", which is trained\n",
      "with private user data and human annotations.\n",
      "For further evaluation, we curate a set of\n",
      "expert-written instructions for novel tasks, and\n",
      "show through human evaluation that tuning\n",
      "GPT3 with SELF-INSTRUCT outperforms using\n",
      "existing public instruction datasets by a large\n",
      "margin, leaving only a 5% absolute gap behind\n",
      "InstructGPT001. SELF-INSTRUCT provides an\n",
      "almost annotation-free method for aligning pretrained language models with instructions, and\n",
      "we release our large synthetic dataset to facilitate future studies on instruction tuning.\n",
      "\n",
      "Annotating large-scale instruction data can be challenging for humans because it requires 1. creativity\n",
      "to come up with novel tasks and 2. expertise for\n",
      "writing the labeled instances for each task. In this\n",
      "section, we detail our process for SELF-INSTRUCT,\n",
      "which refers to the pipeline of generating tasks with\n",
      "a vanilla pretrained language model itself and\n",
      "then conducting instruction tuning with this generated data in order to align the language model to\n",
      "follow instructions better. This pipeline is depicted\n",
      "in Figure 1.\n",
      "\n",
      "6 Discussion and Limitation\n",
      "6.1 Why does SELF-INSTRUCT work?\n",
      "It is worthwhile to reflect on the role that highquality human feedback plays in enabling the recent\n",
      "successes on instruction-tuning LMs (Mishra et al.,\n",
      "2022; Wang et al., 2022; Wei et al., 2022; Sanh\n",
      "et al., 2022; Ouyang et al., 2022). Here are two\n",
      "extreme hypotheses:\n",
      "(1) Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs\n",
      "need to learn about issues that were not quite\n",
      "learned during pre-training.\n",
      "(2) Human feedback is an optional aspect of\n",
      "instruction-tuning as LMs are already quite familiar with instructions from their pre-training.\n",
      "Observing the human feedback is merely a\n",
      "lightweight process for aligning their pretraining distribution/objective which might be\n",
      "replaceable with a different process.\n",
      "While the reality probably lies somewhere in between these two extremes, we conjecture that it is\n",
      "closer to (2)\n",
      ", particularly for larger models. This\n",
      "intuition, that LMs already know much about language instructions, is a key motivation for SELFINSTRUCT and is also supported by its empirical\n",
      "success.\n",
      "\n"
    ],
    "output": [
      "What is the main focus of the passage?",
      "",
      "* The limitations of using large language models for instruction-tuning",
      "* The benefits of using self-instruction for language models",
      "* The process of generating instruction-tuned language models",
      "* The performance of instruction-tuned language models on various tasks",
      "",
      "What is the main goal of the SELF-INSTRUCT framework?",
      "",
      "* To improve the instruction-following capabilities of pretrained language models",
      "* To generate instruction, input, and output samples for language models",
      "* To align pretrained language models with instructions",
      "* To improve the generalization of instruction-tuned language models",
      "",
      "What is the main advantage of the SELF-INSTRUCT framework compared to existing methods?",
      "",
      "* It requires less human annotation",
      "* It provides a nearly annotation-free method for aligning language models with instructions",
      "* It allows for the use of private user data and human annotations",
      "* It requires the use of large-scale instruction data"
    ]
  }
]